{
 "cells": [
  {
   "cell_type": "raw",
   "id": "78a3ffdf-6f67-477d-b509-d0a5347350ad",
   "metadata": {},
   "source": [
    "This Jupyter NoteBook contain 3 sections:\n",
    "\n",
    "Section 1: Initial Code\n",
    "This section includes the initial python code that i started with.\n",
    "\n",
    "Section 2: Code Modification\n",
    "This section includes a the changes made on the code and the reasoning behind it.\n",
    "\n",
    "Section 3 : Final Result\n",
    "This section provides the final code retained for this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62908c2d-840c-430c-96f9-b800d1aed5f1",
   "metadata": {},
   "source": [
    "SECTION 1: INITIAL CODE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72a7daab-9c6c-4196-bd79-bd9aee63f1aa",
   "metadata": {},
   "source": [
    "I chose to start from scratch with simple code generated by prompts to some of the most widely used AI code generators. This approach provided me a valuable opportunity to put into practice the concepts Iâ€™ve recently learned and directly challenge my understanding of the material in real life scenario. Beginning with a simple straightforward baseline from the function description, and some researchs on similar scenarios, allowed me to build confidence and iteratively refine the solution, ensuring a deep grasp of the underlying problem and methods before moving on to more sophisticated techniques.\n",
    "This approach is used for all the functions in this Capstone Competition and provided good results for the majority of the functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdda21c5-037a-4409-9fcd-9273ead141cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial best point: [0.12572016 0.86272469 0.02854433 0.24660527 0.75120624], Sum of negative scores: -2.5711696316081234\n",
      "Iteration 1: Evaluated point: [0.03513832 1.         0.05858456 0.14622461 1.        ], Sum: 1.090289904020251\n",
      "Iteration 2: Evaluated point: [1.         1.         0.58774217 1.         0.63274729], Sum: 0.8704173995185187\n",
      "Iteration 3: Evaluated point: [0.67004758 0.25510479 0.52522885 0.10438031 0.79029202], Sum: 0.4305471638129578\n",
      "Iteration 4: Evaluated point: [0.22588243 0.77684957 0.03562196 0.26939103 0.61963143], Sum: 0.4106657605424981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: Evaluated point: [0.24230104 0.5746347  0.8121854  0.75729344 0.79858772], Sum: 0.4731357740376446\n",
      "Iteration 6: Evaluated point: [0.83690277 0.78568768 0.37800457 0.49963128 0.68297216], Sum: 0.3118958657474306\n",
      "Iteration 7: Evaluated point: [0.45550186 0.1698971  0.16587768 0.4535973  0.31368222], Sum: 0.12906429175913126\n",
      "Iteration 8: Evaluated point: [0.67499165 0.7162088  0.75900934 0.26456011 0.3029492 ], Sum: 0.49264859406748374\n",
      "Iteration 9: Evaluated point: [0.55509224 0.59745917 0.92124195 0.81530382 0.00710274], Sum: 0.7172702773319657\n",
      "Iteration 10: Evaluated point: [0.40763016 0.74413986 0.54953152 0.19840608 0.76064954], Sum: 0.4184462656772053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11: Evaluated point: [0.84721441 0.35815673 0.13141626 0.54519598 0.52674085], Sum: 0.15444773376639856\n",
      "Iteration 12: Evaluated point: [6.07872733e-01 8.11198402e-01 4.44844692e-04 3.84065574e-01\n",
      " 7.35058385e-01], Sum: 0.3723340634826739\n",
      "Iteration 13: Evaluated point: [0.81964295 0.20657572 0.17691664 0.18597584 0.15426797], Sum: 0.4456807233029569\n",
      "Iteration 14: Evaluated point: [0.62851908 0.2303332  0.89215076 0.03699681 0.67008882], Sum: 0.7418493041316577\n",
      "Iteration 15: Evaluated point: [0.18055285 0.07265008 0.9530368  0.20760702 0.70273468], Sum: 0.8307351066499385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16: Evaluated point: [0.52019385 0.75707021 0.69837735 0.88376361 0.75338867], Sum: 0.4313390348983311\n",
      "Iteration 17: Evaluated point: [0.39679558 0.78766812 0.04112548 0.68916352 0.55015164], Sum: 0.23841906454487605\n",
      "Iteration 18: Evaluated point: [0.10367772 0.67756179 0.30452081 0.79389221 0.91390752], Sum: 0.44304596365586335\n",
      "Iteration 19: Evaluated point: [0.0515352  0.34304423 0.9824965  0.72414434 0.91258898], Sum: 0.8558075858358163\n",
      "Iteration 20: Evaluated point: [0.25244353 0.57016824 0.28457859 0.86642865 0.72796542], Sum: 0.21343171181102089\n",
      "Iteration 21: Evaluated point: [0.99975529 0.91493043 0.56433977 0.22177854 0.47952528], Sum: 0.7282549075606763\n",
      "Iteration 22: Evaluated point: [0.79421579 0.19676074 0.32981694 0.71796513 0.82317172], Sum: 0.24711390881551454\n",
      "Iteration 23: Evaluated point: [0.80999118 0.92812334 0.35325396 0.02816406 0.50295287], Sum: 0.7048498426755787\n",
      "Iteration 24: Evaluated point: [0.83393049 0.96489141 0.73375116 0.64379799 0.20729494], Sum: 0.7063464585745935\n",
      "Iteration 25: Evaluated point: [0.35192904 0.1335264  0.87083731 0.58749012 0.65661548], Sum: 0.4434733338250752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26: Evaluated point: [0.6619324  0.29460538 0.87144046 0.81724941 0.54034824], Sum: 0.41269961094801755\n",
      "Iteration 27: Evaluated point: [0.81521218 0.28119414 0.5776072  0.05667801 0.06983965], Sum: 0.6707760248099525\n",
      "Iteration 28: Evaluated point: [0.8880236  0.61832927 0.91140159 0.67336256 0.37879365], Sum: 0.5921149262531981\n",
      "Iteration 29: Evaluated point: [0.26883631 0.1288018  0.09138589 0.25526012 0.09440001], Sum: 0.45386190035016516\n",
      "Iteration 30: Evaluated point: [0.38071189 0.67534472 0.99818321 0.91430313 0.11405574], Sum: 0.8252435900119759\n",
      "Iteration 31: Evaluated point: [0.19195196 0.68748222 0.48681079 0.48730213 0.18068662], Sum: 0.3270997359268224\n",
      "Iteration 32: Evaluated point: [0.1209708  0.76986931 0.72661413 0.24015148 0.73690837], Sum: 0.6480825879740014\n",
      "Iteration 33: Evaluated point: [0.41213587 0.39628185 0.27523626 0.4707722  0.00309792], Sum: 0.27195867517339123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34: Evaluated point: [0.54334972 0.96329164 0.40395565 0.94303457 0.3991249 ], Sum: 0.4578319473160466\n",
      "Iteration 35: Evaluated point: [0.71776095 0.51929849 0.27989419 0.29016816 0.18921866], Sum: 0.2546370099361491\n",
      "Iteration 36: Evaluated point: [0.747281   0.06011586 0.05635413 0.91486475 0.04522634], Sum: 0.5419913222180227\n",
      "Iteration 37: Evaluated point: [0.19751426 0.25749059 0.0787422  0.91980065 0.64855135], Sum: 0.28510152679030476\n",
      "Iteration 38: Evaluated point: [0.46436456 0.34806533 0.00696262 0.77884588 0.83785546], Sum: 0.23597016149681513\n",
      "Iteration 39: Evaluated point: [0.28951149 0.22809204 0.1719093  0.61514934 0.87550765], Sum: 0.23150048241695098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40: Evaluated point: [0.32206723 0.64301004 0.47835254 0.87143669 0.5157728 ], Sum: 0.19645023518252405\n",
      "Iteration 41: Evaluated point: [0.86388789 0.03645258 0.48435346 0.03305664 0.67345084], Sum: 0.6500772865793896\n",
      "Iteration 42: Evaluated point: [0.63392425 0.0803708  0.7487538  0.09134544 0.61383702], Sum: 0.593166836898591\n",
      "Iteration 43: Evaluated point: [0.54448259 0.78617087 0.54811638 0.4000939  0.69183538], Sum: 0.28943163680283757\n",
      "Iteration 44: Evaluated point: [0.75708666 0.70587622 0.51212123 0.62167082 0.97051267], Sum: 0.4265010290861614\n",
      "Iteration 45: Evaluated point: [0.05624738 0.05291433 0.69570635 0.06241625 0.6087087 ], Sum: 0.774782237935915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46: Evaluated point: [0.95385883 0.83017195 0.92397467 0.93525711 0.79176728], Sum: 0.9779056114665591\n",
      "Iteration 47: Evaluated point: [0.54529939 0.73625962 0.08250945 0.9683222  0.76741152], Sum: 0.3695948692280824\n",
      "Iteration 48: Evaluated point: [0.75452609 0.04684134 0.90317715 0.94183359 0.94027687], Sum: 0.864021171901565\n",
      "Iteration 49: Evaluated point: [0.11844805 0.151817   0.84347986 0.33159227 0.42279723], Sum: 0.5805500316172718\n",
      "Iteration 50: Evaluated point: [0.54944224 0.85269627 0.76232138 0.07937229 0.17466659], Sum: 0.7980145428345927\n",
      "\n",
      "Optimal inputs: [0.12572016 0.86272469 0.02854433 0.24660527 0.75120624]\n",
      "Minimum sum of negative scores: -2.5711696316081234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "##########################\n",
    "\n",
    "function = 6\n",
    "# Read the files\n",
    "X_init = np.load(\"initial_inputs.npy\")\n",
    "y_init = np.load(\"initial_outputs.npy\")\n",
    "queries_file = \"queries.txt\"\n",
    "observations_file = \"observations.txt\"\n",
    "\n",
    "# Read queries data\n",
    "import ast\n",
    "queries_data = []\n",
    "with open(queries_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('array(', 'np.array(')\n",
    "        queries_data.append(eval(line.strip()))\n",
    "\n",
    "# Read observations data\n",
    "observations_data = []\n",
    "with open(observations_file, 'r') as f:\n",
    "    for line in f:\n",
    "        observations_data.append(eval(line.strip()))\n",
    "\n",
    "# Extract the specified sub-arrays from queries\n",
    "X = np.array([q[function - 1] for q in queries_data], dtype='float64')\n",
    "y = np.array([o[function - 1] for o in observations_data])\n",
    "\n",
    "# Find and remove duplicates\n",
    "unique_indices = []\n",
    "seen = set()\n",
    "for i, x in enumerate(X):\n",
    "    x_tuple = tuple(x)  # Convert to tuple for hashability\n",
    "    if x_tuple not in seen:\n",
    "        seen.add(x_tuple)\n",
    "        unique_indices.append(i)\n",
    "\n",
    "# Keep only unique queries and observations\n",
    "X_unique = np.concatenate((X_init, X[unique_indices]))\n",
    "y_unique = np.concatenate((y_init, y[unique_indices]))\n",
    "queries_unique = [queries_data[i] for i in unique_indices]\n",
    "observations_unique = [observations_data[i] for i in unique_indices]\n",
    "\n",
    "# Save cleaned data to new files\n",
    "with open(\"queries_unique.txt\", \"w\") as f:\n",
    "    for query in queries_unique:\n",
    "        f.write(str(query) + \"\\n\")\n",
    "\n",
    "with open(\"observations_unique.txt\", \"w\") as f:\n",
    "    for obs in observations_unique:\n",
    "        f.write(str(obs) + \"\\n\")\n",
    "\n",
    "# Save cleaned numpy arrays\n",
    "np.save(\"initial_inputs_unique.npy\", X_unique)\n",
    "np.save(\"initial_outputs_unique.npy\", y_unique)\n",
    "\n",
    "###########################\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'param1': X_unique[:, 0],\n",
    "    'param2': X_unique[:, 1],\n",
    "    'param3': X_unique[:, 2],\n",
    "    'param4': X_unique[:, 3],\n",
    "    'param5': X_unique[:, 4],\n",
    "    'output': y_unique\n",
    "})\n",
    "\n",
    "########################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Black-box function for experimental evaluation\n",
    "def black_box_function(x):\n",
    "    # Replace with your experimental evaluation\n",
    "    # Input: x = [param1, param2, param3, param4, param5] in [0, 1]^5\n",
    "    # Example: Bake cake with these proportions, get negative scores, return sum\n",
    "    #raise NotImplementedError(\"Implement experimental evaluation: bake cake and sum negative scores\")\n",
    "    # For testing, uncomment the synthetic function below:\n",
    "    center = np.array([0.5, 0.4, 0.3, 0.6, 0.5])\n",
    "    return np.sum((x - center) ** 2)\n",
    "\n",
    "# Expected Improvement acquisition function (for minimization)\n",
    "def expected_improvement(X, X_sample, y_sample, gp, xi=0.01):\n",
    "    mu, sigma = gp.predict(X.reshape(-1, 5), return_std=True)\n",
    "    mu_sample_opt = np.min(y_sample)  # Minimize sum of negative scores\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu_sample_opt - mu - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "    return -ei  # Minimize negative EI\n",
    "\n",
    "# Bayesian Optimization using existing data frame\n",
    "def bayesian_optimization(df, n_iter=10):\n",
    "    # Extract features and output from data frame\n",
    "    X = df[['param1', 'param2', 'param3', 'param4', 'param5']].to_numpy()\n",
    "    y = df['output'].to_numpy()\n",
    "    \n",
    "    # Initialize Gaussian Process\n",
    "    kernel = Matern(nu=2.5)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "    gp.fit(X, y)\n",
    "    \n",
    "    bounds = np.array([[0, 1]] * 5)  # Inputs in [0, 1]^5\n",
    "    best_x = X[np.argmin(y)]\n",
    "    best_y = np.min(y)\n",
    "    \n",
    "    print(f\"Initial best point: {best_x}, Sum of negative scores: {best_y}\")\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Optimize acquisition function\n",
    "        x0 = np.random.uniform(0, 1, 5)  # Random starting point\n",
    "        res = minimize(\n",
    "            lambda x: expected_improvement(x, X, y, gp),\n",
    "            x0,\n",
    "            bounds=bounds,\n",
    "            method='L-BFGS-B'\n",
    "        )\n",
    "        x_next = res.x\n",
    "        \n",
    "        # Evaluate black-box function (experimental)\n",
    "        y_next = black_box_function(x_next)\n",
    "        \n",
    "        # Update data frame\n",
    "        new_row = pd.DataFrame({\n",
    "            'param1': [x_next[0]],\n",
    "            'param2': [x_next[1]],\n",
    "            'param3': [x_next[2]],\n",
    "            'param4': [x_next[3]],\n",
    "            'param5': [x_next[4]],\n",
    "            'output': [y_next]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        \n",
    "        # Update X and y\n",
    "        X = df[['param1', 'param2', 'param3', 'param4', 'param5']].to_numpy()\n",
    "        y = df['output'].to_numpy()\n",
    "        \n",
    "        # Refit GP\n",
    "        gp.fit(X, y)\n",
    "        \n",
    "        # Update best\n",
    "        if y_next < best_y:\n",
    "            best_x = x_next\n",
    "            best_y = y_next\n",
    "            print(f\"Iteration {i+1}: New best point: {x_next}, Sum: {y_next}\")\n",
    "        else:\n",
    "            print(f\"Iteration {i+1}: Evaluated point: {x_next}, Sum: {y_next}\")\n",
    "    \n",
    "    return best_x, best_y, df\n",
    "\n",
    "\n",
    "best_x, best_y, updated_df = bayesian_optimization(df, n_iter=50)\n",
    "print(f\"\\nOptimal inputs: {best_x}\")\n",
    "print(f\"Minimum sum of negative scores: {best_y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5b9ac-3665-4025-8831-8375036bd300",
   "metadata": {},
   "source": [
    "SECTION 2: CODE MODIFICATION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6419facc-b2ff-4f79-b8b6-d6e9d150da51",
   "metadata": {},
   "source": [
    "Here, I'm trying to optimize a five-dimensional black-box function for a cake recipe, where the goal was to maximize the sum of negative scores (flavor, consistency, calories, waste, cost) to approach zero, I transformed my initial Bayesian Optimization (BO) code into a more effective solution. The journey from initial to final code addressed key inefficiencies, driven by query results, new knowledge, and practical constraints. Below highlight challenges, improvements, and reasons for each change, reflecting on how I built a better approach.\n",
    "\n",
    "Starting Point: Initial Code and Its Flaws\n",
    "My initial code used BO with a Gaussian Process (GP) and Matern kernel (Î½=2.5) to optimize the cake recipe. It preprocessed data from initial_inputs.npy, initial_outputs.npy, queries.txt, and observations.txt, removing duplicates and creating a DataFrame with param1 to param5 (ingredients) and output (sum of negative scores). The bayesian_optimization function:Fitted a GP and optimized the Expected Improvement (EI) function with a single random restart (x0 = np.random.uniform(0, 1, 5)) in [0, 1]^5.Ran 10 iterations, evaluating the black-box function (a synthetic test function for development), updating the DataFrame, and printing best points/yields.Aimed to minimize the output, incorrectly assuming the goal was to reduce the sum of negative scores.This setup was a decent start but revealed critical flaws during early use.\n",
    "\n",
    "Challenges and Inconsistencies\n",
    "I faced several issues that limited the codeâ€™s effectiveness:\n",
    "Incorrect Optimization Direction: The initial code minimized the output, but the problem required maximizing it (getting closer to zero). Query results showed negative outputs, and minimizing them drove values further from zero, misaligning with the goal.\n",
    "Inefficient Exploration: Single random restarts for EI optimization led to inconsistent suggestions. Queries suggested a complex landscape, but the single-start approach often missed promising regions.\n",
    "Limited Output Information: Outputs only included point coordinates and evaluated scores, lacking GP predictions (mean, uncertainty), making it hard to assess suggestion quality.\n",
    "No Final Suggestion: The code stopped after 10 iterations without suggesting a next point, limiting guidance for further experiments.\n",
    "Fixed Iterations: The rigid 10-iteration loop was impractical for costly baking experiments, especially when queries suggested better results with more iterations.\n",
    "These challenges prompted me to refine the code, leveraging query insights, BO literature, and practical needs.\n",
    "\n",
    "The Journey to Improvement\n",
    "1. Correcting Optimization Direction:\n",
    "Problem: The initial code minimized the output, but the goal was to maximize the sum of negative scores (closer to zero). Query results showed negative values (e.g., -10, -5), and minimizing drove them further negative, worsening performance.\n",
    "Improvement: In the final code, I modified expected_improvement to maximize the output (mu_sample_opt = np.max(y_sample)) and adjusted the EI formula (imp = mu - mu_sample_opt - xi). I updated bayesian_optimization to track the maximum output (best_idx = np.argmax(y)).\n",
    "Impact: This aligned the optimization with the goal, ensuring suggestions improved the score toward zero.\n",
    "Reason: Based on results from the latest queries. Query outputs were consistently negative, and minimizing them was counterproductive, prompting a reevaluation of the problem statement.\n",
    "\n",
    "2. Improved GP Configuration:\n",
    "Problem: The initial GP lacked output normalization, which could destabilize predictions for negative scores with varying scales, as seen in query variability.\n",
    "Improvement: I added normalize_y=True to the GP in bayesian_optimization, standardizing outputs for better model stability.\n",
    "Impact: Enhanced GP accuracy, improving suggestion reliability across diverse query outputs.\n",
    "Reason: New material learned. BO literature emphasized normalizing outputs for GP stability, especially for objectives with varying scales like the cake scores.\n",
    "\n",
    "3. Enhanced Output Reporting:\n",
    "Problem: The initial codeâ€™s outputs (point coordinates, evaluated scores) lacked GP predictions, making it hard to prioritize experiments, especially with inconsistent query results.\n",
    "Improvement: The final code prints predicted outputs for suggested points (y_next = gp.predict(x_next.reshape(1, -1))[0]) and includes a final suggested point with its expected output after iterations.\n",
    "Impact: Provided actionable insights, allowing me to assess suggestion potential before costly experiments.\n",
    "Reason: Combination of query results and new material learned. Query variability highlighted the need for predictive insights, and BO tutorials stressed reporting GP predictions.\n",
    "\n",
    "4. Flexible Iteration Count:\n",
    "Problem: The fixed 10-iteration loop was too rigid for baking experiments, where resources were limited. Queries suggested better results with more iterations.\n",
    "Improvement: I increased n_iter to 20 in the final code, with flexibility to adjust based on needs, and added a final EI optimization to suggest a next point.\n",
    "Impact: Allowed more iterations to explore the complex five-dimensional space while providing a next step, aligning with experimental constraints.\n",
    "Reason: Based on results from the latest queries. Queries showed gradual improvement, suggesting more iterations could refine results further.\n",
    "\n",
    "5. Decoupled Evaluation for Practicality:\n",
    "Problem: The initial code used a synthetic black-box function for testing, bypassing real experimental evaluation, which didnâ€™t reflect the baking process.\n",
    "Improvement: The final code assumes external evaluation (e.g., baking and scoring), using predicted outputs (gp.predict) for development but supporting manual updates.\n",
    "Impact: Made the code practical for real experiments, allowing manual input of baking results.\n",
    "Reason: Practical experimental constraints. Baking experiments required manual scoring, necessitating a workflow that supported external evaluation.\n",
    "\n",
    "Reflection: Building a Better Solution:\n",
    "The journey from initial to final code was about aligning the solution with the complex, five-dimensional cake optimization problem. The initial codeâ€™s incorrect minimization, limited outputs, and rigid structure were like a rough recipeâ€”functional but flawed. Query results exposed the maximization goal and the need for more iterations, while BO literature introduced GP normalization and predictive outputs. Practical baking constraints demanded flexibility. The final code corrects the optimization direction, stabilizes the GP, provides predictive insights, and supports flexible experimentation, making it robust and practical for real-world use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9958d4e-68b3-43e4-a237-ae4652f75e98",
   "metadata": {},
   "source": [
    "SECTION 3: FINAL RESULT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a816efe2-db89-4250-b612-91999f4b79f4",
   "metadata": {},
   "source": [
    "In the final code section, although we did not have access to the actual scores, I meticulously kept records of the expected outputs from my code alongside the observed outputs from each submission. This comparison provided invaluable insights into the direction I needed to take and the changes that should be introduced. Through this process, I learned a great deal, with one of the most significant lessons being that the simplest approach is often the most effective. There is no need to complicate things unless absolutely necessary. If I had to start over and had more time, I would maintain the initial approach but also explore other coders' methods for inspiration, allowing me to experiment with more advanced techniques while still grounding my work in a solid foundational understanding.\n",
    "This approach was applied to all the functions in this Capstone Competition and it was very intuitive and valuable to see where I am without having access to the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f66ee5-ca8b-4738-a0c9-a385f035f042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial best point: [0.475859 0.321227 0.408649 0.763953 0.09598 ], Output (closest to zero): -0.29648469308830117\n",
      "Iteration 1: Evaluated point: [0.2154997  0.1468889  0.04423747 0.1627728  0.5255426 ], Output: -1.8536764091416074\n",
      "Iteration 2: Evaluated point: [0.65006417 0.07880898 0.00723422 0.43022919 0.81821823], Output: -2.101450011285941\n",
      "Iteration 3: New best point: [0.38074692 0.36247126 0.51364881 0.76156949 0.08385686], Output (closest to zero): -0.28929573997502245\n",
      "Iteration 4: Evaluated point: [0.14839893 0.81534725 0.99289857 0.69517801 0.83088771], Output: -1.8027547521341878\n",
      "Iteration 5: Evaluated point: [0.44247364 0.97370001 0.69424653 0.00302634 0.86553429], Output: -2.053930007986769\n",
      "Iteration 6: Evaluated point: [0.02012605 0.44892458 0.70806426 0.31183144 0.27377265], Output: -1.1743262471309681\n",
      "Iteration 7: Evaluated point: [0.65112144 0.77028198 0.5905312  0.97283533 0.33340246], Output: -1.1865184604985322\n",
      "Iteration 8: Evaluated point: [0.22842966 0.95742265 0.76608709 0.02170239 0.16320622], Output: -1.6807605979261269\n",
      "Iteration 9: Evaluated point: [0.2596733  0.0913203  0.86617657 0.61508411 0.47984924], Output: -1.2583983737186977\n",
      "Iteration 10: Evaluated point: [0.80093237 0.08580181 0.94620863 0.01272519 0.96041454], Output: -1.9271620232882258\n",
      "Iteration 11: Evaluated point: [0.84653739 0.43614199 0.77452985 0.54785296 0.25316517], Output: -1.069215713656063\n",
      "Iteration 12: Evaluated point: [0.38797622 0.80695443 0.77571063 0.74472716 0.3682801 ], Output: -1.0315040617699678\n",
      "Iteration 13: Evaluated point: [0.06545412 0.79776159 0.50083697 0.70849933 0.8944753 ], Output: -1.789680782576229\n",
      "Iteration 14: Evaluated point: [0.17886693 0.36541985 0.04243531 0.11243529 0.33818041], Output: -1.712484544573781\n",
      "Iteration 15: Evaluated point: [0.58763049 0.1552182  0.38224693 0.84873571 0.60220284], Output: -1.1210157116818433\n",
      "Iteration 16: Evaluated point: [0.15684827 0.85289121 0.28186052 0.06882192 0.7432015 ], Output: -2.4149037283505175\n",
      "Iteration 17: Evaluated point: [0.70127074 0.984808   0.54931047 0.02564156 0.91112676], Output: -2.1437666478777357\n",
      "Iteration 18: Evaluated point: [0.31995362 0.16701803 0.13013092 0.96774674 0.63319184], Output: -1.350523229930576\n",
      "Iteration 19: Evaluated point: [0.66997488 0.03050925 0.73284905 0.31913874 0.82249964], Output: -1.709690484029891\n",
      "Iteration 20: Evaluated point: [0.10076372 0.08483955 0.17262013 0.48472476 0.75937374], Output: -1.7151077193303006\n",
      "\n",
      "Final best point in dataset: [0.38074692 0.36247126 0.51364881 0.76156949 0.08385686], Output: -0.28929573997502245\n",
      "Next best point to explore: [0.52733754 0.86736801 0.89960478 0.73962891 0.11726609], Expected output: -1.0463385440673618\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "###########################\n",
    "\n",
    "function = 6\n",
    "# Read the files\n",
    "X_init = np.load(\"initial_inputs.npy\")\n",
    "y_init = np.load(\"initial_outputs.npy\")\n",
    "queries_file = \"queries.txt\"\n",
    "observations_file = \"observations.txt\"\n",
    "\n",
    "# Read queries data\n",
    "import ast\n",
    "queries_data = []\n",
    "with open(queries_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('array(', 'np.array(')\n",
    "        queries_data.append(eval(line.strip()))\n",
    "\n",
    "# Read observations data\n",
    "observations_data = []\n",
    "with open(observations_file, 'r') as f:\n",
    "    for line in f:\n",
    "        observations_data.append(eval(line.strip()))\n",
    "\n",
    "# Extract the specified sub-arrays from queries\n",
    "X = np.array([q[function - 1] for q in queries_data], dtype='float64')\n",
    "y = np.array([o[function - 1] for o in observations_data])\n",
    "\n",
    "# Find and remove duplicates\n",
    "unique_indices = []\n",
    "seen = set()\n",
    "for i, x in enumerate(X):\n",
    "    x_tuple = tuple(x)  # Convert to tuple for hashability\n",
    "    if x_tuple not in seen:\n",
    "        seen.add(x_tuple)\n",
    "        unique_indices.append(i)\n",
    "\n",
    "# Keep only unique queries and observations\n",
    "X_unique = np.concatenate((X_init, X[unique_indices]))\n",
    "y_unique = np.concatenate((y_init, y[unique_indices]))\n",
    "queries_unique = [queries_data[i] for i in unique_indices]\n",
    "observations_unique = [observations_data[i] for i in unique_indices]\n",
    "\n",
    "# Save cleaned data to new files\n",
    "with open(\"queries_unique.txt\", \"w\") as f:\n",
    "    for query in queries_unique:\n",
    "        f.write(str(query) + \"\\n\")\n",
    "\n",
    "with open(\"observations_unique.txt\", \"w\") as f:\n",
    "    for obs in observations_unique:\n",
    "        f.write(str(obs) + \"\\n\")\n",
    "\n",
    "# Save cleaned numpy arrays\n",
    "np.save(\"initial_inputs_unique.npy\", X_unique)\n",
    "np.save(\"initial_outputs_unique.npy\", y_unique)\n",
    "#################################\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'param1': X_unique[:, 0],\n",
    "    'param2': X_unique[:, 1],\n",
    "    'param3': X_unique[:, 2],\n",
    "    'param4': X_unique[:, 3],\n",
    "    'param5': X_unique[:, 4],\n",
    "    'output': y_unique\n",
    "})\n",
    "############################\n",
    "\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# Expected Improvement acquisition function (for maximization of output)\n",
    "def expected_improvement(X, X_sample, y_sample, gp, xi=0.01):\n",
    "    mu, sigma = gp.predict(X.reshape(-1, 5), return_std=True)\n",
    "    mu_sample_opt = np.max(y_sample)  # Maximize output (closest to zero since negative)\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "    return -ei  # We minimize negative EI to maximize EI\n",
    "\n",
    "# Bayesian Optimization using existing data frame\n",
    "def bayesian_optimization(df, n_iter=10):\n",
    "    # Extract features and output from data frame\n",
    "    X = df[['param1', 'param2', 'param3', 'param4', 'param5']].to_numpy()\n",
    "    y = df['output'].to_numpy()\n",
    "    \n",
    "    # Initialize Gaussian Process\n",
    "    kernel = Matern(nu=2.5)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, normalize_y=True)\n",
    "    gp.fit(X, y)\n",
    "    \n",
    "    bounds = np.array([[0, 1]] * 5)  # Inputs in [0, 1]^5\n",
    "    best_idx = np.argmax(y)  # Best is max y (closest to zero)\n",
    "    best_x = X[best_idx]\n",
    "    best_y = y[best_idx]\n",
    "    \n",
    "    print(f\"Initial best point: {best_x}, Output (closest to zero): {best_y}\")\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Optimize acquisition function (maximize EI)\n",
    "        x0 = np.random.uniform(0, 1, 5)  # Random starting point\n",
    "        res = minimize(\n",
    "            lambda x: expected_improvement(x, X, y, gp),\n",
    "            x0,\n",
    "            bounds=bounds,\n",
    "            method='L-BFGS-B'\n",
    "        )\n",
    "        x_next = res.x\n",
    "        \n",
    "        # Predict the output for the next point\n",
    "        y_next = gp.predict(x_next.reshape(1, -1))[0]\n",
    "        \n",
    "        # Update data frame\n",
    "        new_row = pd.DataFrame({\n",
    "            'param1': [x_next[0]],\n",
    "            'param2': [x_next[1]],\n",
    "            'param3': [x_next[2]],\n",
    "            'param4': [x_next[3]],\n",
    "            'param5': [x_next[4]],\n",
    "            'output': [y_next]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        \n",
    "        # Update X and y\n",
    "        X = df[['param1', 'param2', 'param3', 'param4', 'param5']].to_numpy()\n",
    "        y = df['output'].to_numpy()\n",
    "        \n",
    "        # Refit GP\n",
    "        gp.fit(X, y)\n",
    "        \n",
    "        # Update best\n",
    "        if y_next > best_y:\n",
    "            best_x = x_next\n",
    "            best_y = y_next\n",
    "            print(f\"Iteration {i+1}: New best point: {x_next}, Output (closest to zero): {y_next}\")\n",
    "        else:\n",
    "            print(f\"Iteration {i+1}: Evaluated point: {x_next}, Output: {y_next}\")\n",
    "    \n",
    "    # Find next best point to explore (optimize acquisition function once more)\n",
    "    x0 = np.random.uniform(0, 1, 5)\n",
    "    res = minimize(\n",
    "        lambda x: expected_improvement(x, X, y, gp),\n",
    "        x0,\n",
    "        bounds=bounds,\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    next_point = res.x\n",
    "    next_point_pred = gp.predict(next_point.reshape(1, -1))[0]\n",
    "    \n",
    "    print(f\"\\nFinal best point in dataset: {best_x}, Output: {best_y}\")\n",
    "    print(f\"Next best point to explore: {next_point}, Expected output: {next_point_pred}\")\n",
    "\n",
    "bayesian_optimization(df, n_iter=20) # changed the number of iterations from 10 to 20 to try to find a better value. looks like working\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
