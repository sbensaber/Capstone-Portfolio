{
 "cells": [
  {
   "cell_type": "raw",
   "id": "78a3ffdf-6f67-477d-b509-d0a5347350ad",
   "metadata": {},
   "source": [
    "This Jupyter NoteBook contain 3 sections:\n",
    "\n",
    "Section 1: Initial Code\n",
    "This section includes the initial python code that i started with.\n",
    "\n",
    "Section 2: Code Modification\n",
    "This section includes a the changes made on the code and the reasoning behind it.\n",
    "\n",
    "Section 3 : Final Result\n",
    "This section provides the final code retained for this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62908c2d-840c-430c-96f9-b800d1aed5f1",
   "metadata": {},
   "source": [
    "SECTION 1: INITIAL CODE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72a7daab-9c6c-4196-bd79-bd9aee63f1aa",
   "metadata": {},
   "source": [
    "I chose to start from scratch with simple code generated by prompts to some of the most widely used AI code generators. This approach provided me a valuable opportunity to put into practice the concepts I’ve recently learned and directly challenge my understanding of the material in real life scenario. Beginning with a simple straightforward baseline from the function description, and some researchs on similar scenarios, allowed me to build confidence and iteratively refine the solution, ensuring a deep grasp of the underlying problem and methods before moving on to more sophisticated techniques.\n",
    "This approach is used for all the functions in this Capstone Competition and provided good results for the majority of the functions.\n",
    "\n",
    "For this fuction, i started to be pretty familiar with the Blackbox function approach. so, i created only 1 code that was actually my initial and final code with the approach that i mention in the \"Code Modification\" section.\n",
    "\n",
    "for consistency, i kept the structure of this document the same but the code in initial section and final section is the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdda21c5-037a-4409-9fcd-9273ead141cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Original Parameters: [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396]\n",
      "Best Original y: 9.7798347140729\n",
      "Best Perturbed Parameters: [0.043432 0.100047 0.223423 0.016864 0.330007 0.313847 0.328406 0.832396]\n",
      "Best Predicted Output: 9.74819948781812\n",
      "Predictions for perturbed points:\n",
      "Point 1: Parameters = [0.       0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.747900\n",
      "Point 2: Parameters = [0.093432 0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.713717\n",
      "Point 3: Parameters = [0.043432 0.050047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.742759\n",
      "Point 4: Parameters = [0.043432 0.150047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.703743\n",
      "Point 5: Parameters = [0.043432 0.100047 0.173423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.738856\n",
      "Point 6: Parameters = [0.043432 0.100047 0.273423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.706913\n",
      "Point 7: Parameters = [0.043432 0.100047 0.223423 0.       0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.740167\n",
      "Point 8: Parameters = [0.043432 0.100047 0.223423 0.066864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.661394\n",
      "Point 9: Parameters = [0.043432 0.100047 0.223423 0.016864 0.280007 0.363847 0.328406 0.832396], Predicted Output = 9.731826\n",
      "Point 10: Parameters = [0.043432 0.100047 0.223423 0.016864 0.380007 0.363847 0.328406 0.832396], Predicted Output = 9.744572\n",
      "Point 11: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.313847 0.328406 0.832396], Predicted Output = 9.748199\n",
      "Point 12: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.413847 0.328406 0.832396], Predicted Output = 9.710743\n",
      "Point 13: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.278406 0.832396], Predicted Output = 9.685440\n",
      "Point 14: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.378406 0.832396], Predicted Output = 9.697395\n",
      "Point 15: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.782396], Predicted Output = 9.747623\n",
      "Point 16: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.882396], Predicted Output = 9.738661\n",
      "\n",
      "Best Point:\n",
      "Index: 11\n",
      "Parameters: [0.043432 0.100047 0.223423 0.016864 0.330007 0.313847 0.328406 0.832396]\n",
      "Best Predicted Output: 9.748199\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "##########################\n",
    "\n",
    "function = 8\n",
    "# Read the files\n",
    "X_init = np.load(\"initial_inputs.npy\")\n",
    "y_init = np.load(\"initial_outputs.npy\")\n",
    "queries_file = \"queries.txt\"\n",
    "observations_file = \"observations.txt\"\n",
    "\n",
    "# Read queries data\n",
    "import ast\n",
    "queries_data = []\n",
    "with open(queries_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('array(', 'np.array(')\n",
    "        queries_data.append(eval(line.strip()))\n",
    "\n",
    "# Read observations data\n",
    "observations_data = []\n",
    "with open(observations_file, 'r') as f:\n",
    "    for line in f:\n",
    "        observations_data.append(eval(line.strip()))\n",
    "\n",
    "# Extract the specified sub-arrays from queries\n",
    "X = np.array([q[function - 1] for q in queries_data], dtype='float64')\n",
    "y = np.array([o[function - 1] for o in observations_data])\n",
    "\n",
    "# Find and remove duplicates\n",
    "unique_indices = []\n",
    "seen = set()\n",
    "for i, x in enumerate(X):\n",
    "    x_tuple = tuple(x)  # Convert to tuple for hashability\n",
    "    if x_tuple not in seen:\n",
    "        seen.add(x_tuple)\n",
    "        unique_indices.append(i)\n",
    "\n",
    "# Keep only unique queries and observations\n",
    "X_unique = np.concatenate((X_init, X[unique_indices]))\n",
    "y_unique = np.concatenate((y_init, y[unique_indices]))\n",
    "queries_unique = [queries_data[i] for i in unique_indices]\n",
    "observations_unique = [observations_data[i] for i in unique_indices]\n",
    "\n",
    "# Save cleaned data to new files\n",
    "with open(\"queries_unique.txt\", \"w\") as f:\n",
    "    for query in queries_unique:\n",
    "        f.write(str(query) + \"\\n\")\n",
    "\n",
    "with open(\"observations_unique.txt\", \"w\") as f:\n",
    "    for obs in observations_unique:\n",
    "        f.write(str(obs) + \"\\n\")\n",
    "\n",
    "# Save cleaned numpy arrays\n",
    "np.save(\"initial_inputs_unique.npy\", X_unique)\n",
    "np.save(\"initial_outputs_unique.npy\", y_unique)\n",
    "#######################\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'param1': X_unique[:, 0],\n",
    "    'param2': X_unique[:, 1],\n",
    "    'param3': X_unique[:, 2],\n",
    "    'param4': X_unique[:, 3],\n",
    "    'param5': X_unique[:, 4],\n",
    "    'param6': X_unique[:, 5],\n",
    "    'param7': X_unique[:, 6],\n",
    "    'param8': X_unique[:, 7],\n",
    "    'output': y_unique\n",
    "})\n",
    "\n",
    "############################\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "X = df[['param1', 'param2', 'param3', 'param4', 'param5', 'param6', 'param7', 'param8']].values\n",
    "y = df['output'].values\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Find the index of the highest actual 'y'\n",
    "best_idx = np.argmax(y)\n",
    "\n",
    "# Get the corresponding parameters and actual 'y'\n",
    "best_params = X[best_idx]\n",
    "best_y = y[best_idx]\n",
    "\n",
    "# Generate perturbed points\n",
    "perturbed_points = []\n",
    "for i in range(8):\n",
    "    for delta in [-0.05, 0.05]:\n",
    "        new_params = best_params.copy()  # Use best_params instead of best_point\n",
    "        new_params[i] = min(max(new_params[i] + delta, 0), 1)  # Perturb one parameter\n",
    "        perturbed_points.append(new_params)\n",
    "\n",
    "# Predict outputs for perturbed points\n",
    "predictions = rf.predict(np.array(perturbed_points))\n",
    "best_idx = np.argmax(predictions)\n",
    "best_new_params = perturbed_points[best_idx]\n",
    "best_predicted_output = predictions[best_idx]\n",
    "\n",
    "# Print results\n",
    "print(\"Best Original Parameters:\", best_params)\n",
    "print(\"Best Original y:\", best_y)\n",
    "print(\"Best Perturbed Parameters:\", best_new_params)\n",
    "print(\"Best Predicted Output:\", best_predicted_output)\n",
    "\n",
    "###########################\n",
    "\n",
    "# Print the results\n",
    "print(\"Predictions for perturbed points:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Point {i + 1}: Parameters = {perturbed_points[i]}, Predicted Output = {pred:.6f}\")\n",
    "\n",
    "print(\"\\nBest Point:\")\n",
    "print(f\"Index: {best_idx + 1}\")\n",
    "print(f\"Parameters: {best_new_params}\")\n",
    "print(f\"Best Predicted Output: {best_predicted_output:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5b9ac-3665-4025-8831-8375036bd300",
   "metadata": {},
   "source": [
    "SECTION 2: CODE MODIFICATION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6419facc-b2ff-4f79-b8b6-d6e9d150da51",
   "metadata": {},
   "source": [
    "To tackle this high-dimensional black-box optimization problem in an 8-dimensional search space, we aim to find the parameter values (param1 to param8) that maximize the output. Given the complexity of high-dimensional spaces and the suggestion to focus on local solutions, a local optimization approach around the best-known points is reasonable. The dataset provides 90 samples with 8 input parameters (all in [0, 1]) and corresponding outputs. Let’s proceed with a structured approach.\n",
    "\n",
    "Step 1: Analyze the Data\n",
    "First, identify the point with the highest output to use as a starting point for local optimization, as the problem emphasizes local solutions. The dataset’s maximum output is:\n",
    "Index 51: Output = 9.755085, Parameters = [0.043432, 0.100047, 0.223423, 0.016864, 0.280007, 0.363847, 0.328406, 0.832396]\n",
    "\n",
    "This point is the best candidate for initiating a local search, as it has the highest output in the dataset.\n",
    "\n",
    "Step 2: Local Optimization Strategy\n",
    "High-dimensional black-box optimization is challenging due to the curse of dimensionality, where the search space grows exponentially. Since the problem suggests sticking to local solutions, we can use a local optimization method like gradient-free optimization (e.g., Nelder-Mead or hill-climbing) or a simple grid search around the best point. However, since the function is a black box (no analytical form or gradients provided), we’ll assume the dataset samples are representative and use a data-driven approach, such as:\n",
    "Local Grid Search: Perturb the parameters around the best point (index 51) within a small radius and evaluate hypothetical outputs by interpolating or modeling the function based on the dataset.\n",
    "\n",
    "Surrogate Model: Build a simple model (e.g., Gaussian Process or Random Forest) to approximate the black-box function and optimize within a local region.\n",
    "\n",
    "Given the dataset size (90 points) and the need for simplicity, let’s start with a local grid search around the best point, as it’s computationally feasible and aligns with the “local solutions” hint. If needed, we can later fit a surrogate model for more precise optimization.\n",
    "Step 3: Local Grid Search\n",
    "We’ll perturb each parameter of the best point (index 51) by a small amount, say ±0.05, while ensuring parameters stay within [0, 1]. The starting point is:\n",
    "Parameters: [0.043432, 0.100047, 0.223423, 0.016864, 0.280007, 0.363847, 0.328406, 0.832396]\n",
    "\n",
    "Output: 9.755085\n",
    "\n",
    "For each parameter, create a grid with 3 points: original value, value + 0.05, and value - 0.05 (clipped to [0, 1]). This creates a local grid of up to 3^8 = 6561 points, which is too large for manual evaluation without the black-box function. Instead, we’ll reduce the grid by perturbing one parameter at a time (8 directions, ±0.05), yielding 16 points to evaluate. Since we don’t have the actual black-box function, we’ll estimate outputs using a surrogate model trained on the dataset.\n",
    "Step 4: Surrogate Model\n",
    "To estimate outputs for new points, train a simple model on the dataset. A Random Forest Regressor is suitable for black-box problems due to its ability to capture non-linear relationships. Using the dataset:\n",
    "Features: param1 to param8 (90 rows, 8 columns)\n",
    "\n",
    "Target: output (90 values)\n",
    "\n",
    "Fit a Random Forest model (e.g., using Python’s scikit-learn, but here we’ll describe the process):\n",
    "Train the Model: Use the 90 samples to train a Random Forest with default parameters (e.g., 100 trees).\n",
    "\n",
    "Evaluate Grid Points: For each of the 16 perturbed points around index 51, predict the output using the model.\n",
    "\n",
    "Perturbed points (clipped to [0, 1]):\n",
    "param1: [0, 0.043432, 0.093432]\n",
    "\n",
    "param2: [0.050047, 0.100047, 0.150047]\n",
    "\n",
    "param3: [0.173423, 0.223423, 0.273423]\n",
    "\n",
    "param4: [0, 0.016864, 0.066864]\n",
    "\n",
    "param5: [0.230007, 0.280007, 0.330007]\n",
    "\n",
    "param6: [0.313847, 0.363847, 0.413847]\n",
    "\n",
    "param7: [0.278406, 0.328406, 0.378406]\n",
    "\n",
    "param8: [0.782396, 0.832396, 0.882396]\n",
    "\n",
    "Test combinations where one parameter is perturbed at a time:\n",
    "Point 1: [0, 0.100047, 0.223423, 0.016864, 0.280007, 0.363847, 0.328406, 0.832396]\n",
    "\n",
    "Point 2: [0.093432, 0.100047, 0.223423, 0.016864, 0.280007, 0.363847, 0.328406, 0.832396]\n",
    "\n",
    "... (similarly for param2 to param8, ±0.05)\n",
    "\n",
    "Step 5: Model-Based Prediction\n",
    "Since we can’t compute the actual black-box function, assume the Random Forest predicts outputs for these points. In practice, you’d implement this in Python:\n",
    "\n",
    "Step 6: Alternative Approach\n",
    "If the grid search yields insufficient improvement, consider other local optimization methods:\n",
    "Nelder-Mead: Use the best point as the initial simplex and optimize locally.\n",
    "\n",
    "Bayesian Optimization: Use a Gaussian Process to model the function and focus on promising regions near the best point.\n",
    "\n",
    "However, given the dataset and the emphasis on local solutions, the grid search with a surrogate model is sufficient for an initial solution.\n",
    "Step 7: Final Answer\n",
    "Without the actual black-box function, we can’t compute exact outputs. Based on the dataset, the best-known point is:\n",
    "Parameters: [0.043432, 0.100047, 0.223423, 0.016864, 0.280007, 0.363847, 0.328406, 0.832396]\n",
    "\n",
    "Output: 9.755085\n",
    "\n",
    "To improve this, perform a local grid search around this point with ±0.05 perturbations, using a Random Forest to estimate outputs. The best predicted point from the model would be the solution. F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9958d4e-68b3-43e4-a237-ae4652f75e98",
   "metadata": {},
   "source": [
    "SECTION 3: FINAL RESULT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a816efe2-db89-4250-b612-91999f4b79f4",
   "metadata": {},
   "source": [
    "In the final code section, although we did not have access to the actual scores, I meticulously kept records of the expected outputs from my code alongside the observed outputs from each submission. This comparison provided invaluable insights into the direction I needed to take and the changes that should be introduced. Through this process, I learned a great deal, with one of the most significant lessons being that the simplest approach is often the most effective. There is no need to complicate things unless absolutely necessary. If I had to start over and had more time, I would maintain the initial approach but also explore other coders' methods for inspiration, allowing me to experiment with more advanced techniques while still grounding my work in a solid foundational understanding.\n",
    "This approach was applied to all the functions in this Capstone Competition and it was very intuitive and valuable to see where I am without having access to the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f66ee5-ca8b-4738-a0c9-a385f035f042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Original Parameters: [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396]\n",
      "Best Original y: 9.7798347140729\n",
      "Best Perturbed Parameters: [0.043432 0.100047 0.223423 0.016864 0.330007 0.313847 0.328406 0.832396]\n",
      "Best Predicted Output: 9.74819948781812\n",
      "Predictions for perturbed points:\n",
      "Point 1: Parameters = [0.       0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.747900\n",
      "Point 2: Parameters = [0.093432 0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.713717\n",
      "Point 3: Parameters = [0.043432 0.050047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.742759\n",
      "Point 4: Parameters = [0.043432 0.150047 0.223423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.703743\n",
      "Point 5: Parameters = [0.043432 0.100047 0.173423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.738856\n",
      "Point 6: Parameters = [0.043432 0.100047 0.273423 0.016864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.706913\n",
      "Point 7: Parameters = [0.043432 0.100047 0.223423 0.       0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.740167\n",
      "Point 8: Parameters = [0.043432 0.100047 0.223423 0.066864 0.330007 0.363847 0.328406 0.832396], Predicted Output = 9.661394\n",
      "Point 9: Parameters = [0.043432 0.100047 0.223423 0.016864 0.280007 0.363847 0.328406 0.832396], Predicted Output = 9.731826\n",
      "Point 10: Parameters = [0.043432 0.100047 0.223423 0.016864 0.380007 0.363847 0.328406 0.832396], Predicted Output = 9.744572\n",
      "Point 11: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.313847 0.328406 0.832396], Predicted Output = 9.748199\n",
      "Point 12: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.413847 0.328406 0.832396], Predicted Output = 9.710743\n",
      "Point 13: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.278406 0.832396], Predicted Output = 9.685440\n",
      "Point 14: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.378406 0.832396], Predicted Output = 9.697395\n",
      "Point 15: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.782396], Predicted Output = 9.747623\n",
      "Point 16: Parameters = [0.043432 0.100047 0.223423 0.016864 0.330007 0.363847 0.328406 0.882396], Predicted Output = 9.738661\n",
      "\n",
      "Best Point:\n",
      "Index: 11\n",
      "Parameters: [0.043432 0.100047 0.223423 0.016864 0.330007 0.313847 0.328406 0.832396]\n",
      "Best Predicted Output: 9.748199\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "##########################\n",
    "\n",
    "function = 8\n",
    "# Read the files\n",
    "X_init = np.load(\"initial_inputs.npy\")\n",
    "y_init = np.load(\"initial_outputs.npy\")\n",
    "queries_file = \"queries.txt\"\n",
    "observations_file = \"observations.txt\"\n",
    "\n",
    "# Read queries data\n",
    "import ast\n",
    "queries_data = []\n",
    "with open(queries_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('array(', 'np.array(')\n",
    "        queries_data.append(eval(line.strip()))\n",
    "\n",
    "# Read observations data\n",
    "observations_data = []\n",
    "with open(observations_file, 'r') as f:\n",
    "    for line in f:\n",
    "        observations_data.append(eval(line.strip()))\n",
    "\n",
    "# Extract the specified sub-arrays from queries\n",
    "X = np.array([q[function - 1] for q in queries_data], dtype='float64')\n",
    "y = np.array([o[function - 1] for o in observations_data])\n",
    "\n",
    "# Find and remove duplicates\n",
    "unique_indices = []\n",
    "seen = set()\n",
    "for i, x in enumerate(X):\n",
    "    x_tuple = tuple(x)  # Convert to tuple for hashability\n",
    "    if x_tuple not in seen:\n",
    "        seen.add(x_tuple)\n",
    "        unique_indices.append(i)\n",
    "\n",
    "# Keep only unique queries and observations\n",
    "X_unique = np.concatenate((X_init, X[unique_indices]))\n",
    "y_unique = np.concatenate((y_init, y[unique_indices]))\n",
    "queries_unique = [queries_data[i] for i in unique_indices]\n",
    "observations_unique = [observations_data[i] for i in unique_indices]\n",
    "\n",
    "# Save cleaned data to new files\n",
    "with open(\"queries_unique.txt\", \"w\") as f:\n",
    "    for query in queries_unique:\n",
    "        f.write(str(query) + \"\\n\")\n",
    "\n",
    "with open(\"observations_unique.txt\", \"w\") as f:\n",
    "    for obs in observations_unique:\n",
    "        f.write(str(obs) + \"\\n\")\n",
    "\n",
    "# Save cleaned numpy arrays\n",
    "np.save(\"initial_inputs_unique.npy\", X_unique)\n",
    "np.save(\"initial_outputs_unique.npy\", y_unique)\n",
    "#######################\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'param1': X_unique[:, 0],\n",
    "    'param2': X_unique[:, 1],\n",
    "    'param3': X_unique[:, 2],\n",
    "    'param4': X_unique[:, 3],\n",
    "    'param5': X_unique[:, 4],\n",
    "    'param6': X_unique[:, 5],\n",
    "    'param7': X_unique[:, 6],\n",
    "    'param8': X_unique[:, 7],\n",
    "    'output': y_unique\n",
    "})\n",
    "\n",
    "############################\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "X = df[['param1', 'param2', 'param3', 'param4', 'param5', 'param6', 'param7', 'param8']].values\n",
    "y = df['output'].values\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Find the index of the highest actual 'y'\n",
    "best_idx = np.argmax(y)\n",
    "\n",
    "# Get the corresponding parameters and actual 'y'\n",
    "best_params = X[best_idx]\n",
    "best_y = y[best_idx]\n",
    "\n",
    "# Generate perturbed points\n",
    "perturbed_points = []\n",
    "for i in range(8):\n",
    "    for delta in [-0.05, 0.05]:\n",
    "        new_params = best_params.copy()  # Use best_params instead of best_point\n",
    "        new_params[i] = min(max(new_params[i] + delta, 0), 1)  # Perturb one parameter\n",
    "        perturbed_points.append(new_params)\n",
    "\n",
    "# Predict outputs for perturbed points\n",
    "predictions = rf.predict(np.array(perturbed_points))\n",
    "best_idx = np.argmax(predictions)\n",
    "best_new_params = perturbed_points[best_idx]\n",
    "best_predicted_output = predictions[best_idx]\n",
    "\n",
    "# Print results\n",
    "print(\"Best Original Parameters:\", best_params)\n",
    "print(\"Best Original y:\", best_y)\n",
    "print(\"Best Perturbed Parameters:\", best_new_params)\n",
    "print(\"Best Predicted Output:\", best_predicted_output)\n",
    "\n",
    "###########################\n",
    "\n",
    "# Print the results\n",
    "print(\"Predictions for perturbed points:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Point {i + 1}: Parameters = {perturbed_points[i]}, Predicted Output = {pred:.6f}\")\n",
    "\n",
    "print(\"\\nBest Point:\")\n",
    "print(f\"Index: {best_idx + 1}\")\n",
    "print(f\"Parameters: {best_new_params}\")\n",
    "print(f\"Best Predicted Output: {best_predicted_output:.6f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
